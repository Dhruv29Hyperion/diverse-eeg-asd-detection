{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import spkit\n",
    "from scipy.fft import fft\n",
    "from scipy.signal import welch\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some random variables\n",
    "BLOCKS = 20\n",
    "RUNS = 10\n",
    "EVENTS = 8\n",
    "DATA_FOLDER = 'data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_artifacts_with_atar(signals):\n",
    "    \"\"\"\n",
    "    Apply ATAR (Artifact Removal) algorithm to the signals.\n",
    "    Args:\n",
    "        signals: 2D numpy array of shape (channels, timepoints) for 1 trial/block\n",
    "    Returns:\n",
    "        Cleaned signals: 2D numpy array of the same shape after artifact removal\n",
    "    \"\"\"\n",
    "    cleaned_signals = np.zeros_like(signals)\n",
    "    # If it's a 2D array, just apply ATAR across it\n",
    "    cleaned_signals = spkit.eeg.ATAR(signals, method='adaptive')\n",
    "    \n",
    "    return cleaned_signals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_session_signals(session_path):\n",
    "    # Load signals and targets\n",
    "    train_data_signals = loadmat(os.path.join(session_path, 'trainData.mat'))['trainData']\n",
    "    train_data_targets = np.loadtxt(os.path.join(session_path, 'trainTargets.txt'))\n",
    "\n",
    "    # Apply ATAR artifact removal and filter signals\n",
    "    filtered_signals = train_data_signals[:, :, train_data_targets[:] == 1]\n",
    "\n",
    "    # Calculate mean of events every 10 runs\n",
    "    filtered_signals = np.mean(\n",
    "        filtered_signals.reshape(filtered_signals.shape[0], filtered_signals.shape[1], -1, RUNS),\n",
    "        axis=3\n",
    "    )\n",
    "\n",
    "    # Mean across all epochs\n",
    "    processed_signals = np.mean(filtered_signals, axis=1)\n",
    "\n",
    "    return processed_signals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing participant: SBJ01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naraazanda/Desktop/Anshuman/DL_Project/experiments/venv/lib/python3.12/site-packages/spkit/eeg/atar_algorithm.py:1117: UserWarning: Make sure the upper and lower bound values (k1,k2) are of same order as signal amplitude. If amplitude of signal is much lower than k2 or even k1, ATAR algorithm will have no affect on signal. For example, k2=100, and/or k1=10 is setting for amplitude in micro-volt (in order of 100s). If provided signal is in volt (1e-6), multiply signal with 1e6 (X*1e6) and then apply ATAR\n",
      "/Users/naraazanda/Desktop/Anshuman/DL_Project/experiments/venv/lib/python3.12/site-packages/spkit/eeg/atar_algorithm.py:1120: UserWarning: Upper bound k2 is set to very high. ATAR might have no impact of signal. Either change amplitude unit of signal by multiplying 1e3, or 1e6, or lower the value of k2 and respectively, k1.\n",
      "/Users/naraazanda/Desktop/Anshuman/DL_Project/experiments/venv/lib/python3.12/site-packages/spkit/eeg/atar_algorithm.py:510: UserWarning: Upper bound k2 is set to very high. ATAR might have no impact of signal. Either change amplitude unit of signal by multiplying 1e3, or 1e6, or lower the value of k2 and respectively, k1.  One of the straightforward way to set k2 is k2 = np.std(X).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing participant: SBJ02\n",
      "Processing participant: SBJ03\n",
      "Processing participant: SBJ04\n",
      "Processing participant: SBJ05\n",
      "Processing participant: SBJ06\n",
      "Processing participant: SBJ07\n",
      "Processing participant: SBJ08\n",
      "Processing participant: SBJ09\n",
      "Processing participant: SBJ10\n",
      "Processing participant: SBJ11\n",
      "Processing participant: SBJ12\n",
      "Processing participant: SBJ13\n",
      "Processing participant: SBJ14\n",
      "Processing participant: SBJ15\n",
      "Shape of final aggregated data:  (15, 8, 20)\n"
     ]
    }
   ],
   "source": [
    "participants = [f'SBJ{i:02d}' for i in range(1, 16)]  \n",
    "\n",
    "# Initialize an empty list to store data for all participants\n",
    "all_participants_data = []\n",
    "\n",
    "for participant in participants:\n",
    "    print(f'Processing participant: {participant}')\n",
    "    participant_folder = os.path.join(DATA_FOLDER, participant)\n",
    "\n",
    "    # Iterate over all sessions for the current participant\n",
    "    session_folders = [os.path.join(participant_folder, f'S0{i}/Train') for i in range(1, 8)]\n",
    "\n",
    "    participant_data = np.zeros((EVENTS, BLOCKS))\n",
    "\n",
    "    for session_path in session_folders:\n",
    "        session_data = process_session_signals(session_path)\n",
    "        participant_data += session_data\n",
    "\n",
    "    # Take the mean across all sessions for the participant\n",
    "    participant_data /= len(session_folders)\n",
    "\n",
    "    # Apply ATAR (Artifact Removal) algorithm to the signals.\n",
    "    participant_data = remove_artifacts_with_atar(participant_data)\n",
    "\n",
    "    # Append the participant data\n",
    "    all_participants_data.append(participant_data)\n",
    "\n",
    "final_data = np.array(all_participants_data)\n",
    "print(\"Shape of final aggregated data: \", final_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the frequency bands\n",
    "freq_bands = [(0, 4), (4, 8), (8, 13), (13, 30), (30, 50)]  # Delta, Theta, Alpha, Beta, Gamma\n",
    "\n",
    "def extract_frequency_domain_features(signals):\n",
    "    \"\"\"\n",
    "    Extract frequency-domain features using FFT.\n",
    "    \n",
    "    Args:\n",
    "        signals: 3D numpy array of shape (channels, timepoints, trials)\n",
    "    \n",
    "    Returns:\n",
    "        freq_features: 2D numpy array of shape (channels, len(freq_bands))\n",
    "    \"\"\"\n",
    "    freq_features = np.zeros((signals.shape[0], len(freq_bands)))\n",
    "    \n",
    "    for ch in range(signals.shape[0]):\n",
    "        for trial in range(signals.shape[2]):\n",
    "            signal = signals[ch, :, trial]\n",
    "            \n",
    "            # Apply FFT to the signal\n",
    "            fft_result = np.abs(np.fft.fft(signal))\n",
    "            \n",
    "            # Average power in each frequency band\n",
    "            for i, (f_start, f_end) in enumerate(freq_bands):\n",
    "                band_power = np.mean(fft_result[int(f_start):int(f_end)])\n",
    "                freq_features[ch, i] += band_power\n",
    "                \n",
    "    freq_features /= signals.shape[2]\n",
    "    \n",
    "    return freq_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Frequency-domain features shape for participant 1: (8, 5)\n",
      "  Frequency-domain features shape for participant 2: (8, 5)\n",
      "  Frequency-domain features shape for participant 3: (8, 5)\n",
      "  Frequency-domain features shape for participant 4: (8, 5)\n",
      "  Frequency-domain features shape for participant 5: (8, 5)\n",
      "  Frequency-domain features shape for participant 6: (8, 5)\n",
      "  Frequency-domain features shape for participant 7: (8, 5)\n",
      "  Frequency-domain features shape for participant 8: (8, 5)\n",
      "  Frequency-domain features shape for participant 9: (8, 5)\n",
      "  Frequency-domain features shape for participant 10: (8, 5)\n",
      "  Frequency-domain features shape for participant 11: (8, 5)\n",
      "  Frequency-domain features shape for participant 12: (8, 5)\n",
      "  Frequency-domain features shape for participant 13: (8, 5)\n",
      "  Frequency-domain features shape for participant 14: (8, 5)\n",
      "  Frequency-domain features shape for participant 15: (8, 5)\n"
     ]
    }
   ],
   "source": [
    "all_participants_features = []\n",
    "\n",
    "for participant_id in range(15):  # Assuming 15 participants\n",
    "    # Simulated participant data\n",
    "    participant_data = np.random.rand(8, 256, 20)  # (8 channels, 256 timepoints, 20 trials)\n",
    "    \n",
    "    # Extract frequency-domain features\n",
    "    try:\n",
    "        freq_features = extract_frequency_domain_features(participant_data)\n",
    "        all_participants_features.append(freq_features)\n",
    "        print(f\"  Frequency-domain features shape for participant {participant_id + 1}: {freq_features.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing participant {participant_id + 1}: {e}\")\n",
    "        \n",
    "all_features_array = np.stack(all_participants_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hjorth_parameters(signal):\n",
    "    \"\"\"\n",
    "    Calculate Hjorth activity, mobility, and complexity for a signal.\n",
    "    \"\"\"\n",
    "    variance = np.var(signal)\n",
    "    diff_signal = np.diff(signal)\n",
    "    activity = variance\n",
    "    mobility = np.std(diff_signal) / np.std(signal)\n",
    "    complexity = mobility / (np.std(np.diff(diff_signal)) / np.std(diff_signal))\n",
    "    return activity, mobility, complexity\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def calculate_skewness_kurtosis(signal):\n",
    "    \"\"\"\n",
    "    Calculate skewness and kurtosis for a signal.\n",
    "    \"\"\"\n",
    "    return skew(signal), kurtosis(signal)\n",
    "\n",
    "def calculate_peak_to_peak(signal):\n",
    "    \"\"\"\n",
    "    Calculate the peak-to-peak value for a signal.\n",
    "    \"\"\"\n",
    "    return np.ptp(signal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to participant_features.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the column headers\n",
    "columns = [\n",
    "    \"participant_id\", \"channel\", \"Autistic\",\n",
    "    \"mean\", \"variance\", \"rms\", \"hjorth_activity\", \"hjorth_mobility\", \n",
    "    \"hjorth_complexity\", \"skewness\", \"kurtosis\", \"peak_to_peak\", \n",
    "    \"delta_power\", \"theta_power\", \"alpha_power\", \"beta_power\", \n",
    "    \"spectral_entropy\"\n",
    "]\n",
    "\n",
    "# Prepare data for saving\n",
    "csv_data = []\n",
    "participant_ids = [f\"SBJ{i:02d}\" for i in range(1, 16)]\n",
    "\n",
    "for participant_index, participant_data in enumerate(all_participants_features):\n",
    "    participant_id = participant_ids[participant_index]\n",
    "    autistic_label = 1  # Adjust based on your dataset\n",
    "    \n",
    "    for channel_index in range(participant_data.shape[0]):  # Iterate over channels\n",
    "        # Combine time-dependent and frequency-domain features\n",
    "        time_features = []\n",
    "\n",
    "        signal = participant_data[channel_index]\n",
    "        mean = np.mean(signal)\n",
    "        variance = np.var(signal)\n",
    "        rms = np.sqrt(np.mean(signal**2))\n",
    "        hjorth_activity, hjorth_mobility, hjorth_complexity = hjorth_parameters(signal)\n",
    "        skewness, kurtosis_value = calculate_skewness_kurtosis(signal)\n",
    "        peak_to_peak = calculate_peak_to_peak(signal)\n",
    "\n",
    "        time_features = [mean, variance, rms, hjorth_activity, hjorth_mobility, hjorth_complexity, skewness, kurtosis_value, peak_to_peak]\n",
    "\n",
    "        \n",
    "        # Frequency-domain features\n",
    "        freq_features = participant_data[channel_index]  # Directly from all_participants_features\n",
    "        \n",
    "        # Combine all features\n",
    "        channel_features = time_features + list(freq_features)\n",
    "\n",
    "        # Check for feature length consistency\n",
    "        if len(channel_features) != len(columns) - 3:  # 3 non-feature columns (ID, channel, label)\n",
    "            print(f\"Feature length mismatch for participant {participant_id}, channel {channel_index}: {len(channel_features)}\")\n",
    "        \n",
    "        # Append the features to the csv_data\n",
    "        csv_data.append([\n",
    "            participant_id, \n",
    "            channel_index, \n",
    "            autistic_label, \n",
    "            *channel_features  # Combine all features\n",
    "        ])\n",
    "\n",
    "# Ensure consistency in the data structure\n",
    "for row in csv_data:\n",
    "    assert len(row) == len(columns), f\"Row length mismatch: {len(row)} vs {len(columns)}\"\n",
    "\n",
    "# Create a DataFrame and save to CSV\n",
    "df = pd.DataFrame(csv_data, columns=columns)\n",
    "output_path = \"participant_features.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Features saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
